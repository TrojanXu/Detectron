 first attempt, retinanet without aug 
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.378
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.573
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.409
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.197
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.421
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.510
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.320
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.504
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.532
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.317
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.581
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.704
INFO task_evaluation.py: 180: copypaste: Dataset: coco_2014_minival
INFO task_evaluation.py: 182: copypaste: Task: box
INFO task_evaluation.py: 185: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 186: copypaste: 0.3779,0.5732,0.4090,0.1973,0.4215,0.5101


conclusion, cannot use aug in retinanet, and reported from community, cannot use simple_infer script in retinanet

===================================
faster rcnn test, see if aug can work
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.413
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.637
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.447
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.255
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.453
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.529
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.331
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.521
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.546
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.361
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.586
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.680
INFO json_dataset_evaluator.py: 199: Wrote json eval results to: ./test/coco_2014_minival/generalized_rcnn/detection_results.pkl
INFO task_evaluation.py:  61: Evaluating bounding boxes is done!
INFO task_evaluation.py: 180: copypaste: Dataset: coco_2014_minival
INFO task_evaluation.py: 182: copypaste: Task: box
INFO task_evaluation.py: 185: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 186: copypaste: 0.4126,0.6368,0.4472,0.2546,0.4527,0.5285

with aug
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.440
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.654
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.484
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.295
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.472
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.564
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.349
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.569
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.605
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.450
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.638
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.741
INFO json_dataset_evaluator.py: 199: Wrote json eval results to: ./test/coco_2014_minival/generalized_rcnn/detection_results.pkl
INFO task_evaluation.py:  61: Evaluating bounding boxes is done!
INFO task_evaluation.py: 180: copypaste: Dataset: coco_2014_minival
INFO task_evaluation.py: 182: copypaste: Task: box
INFO task_evaluation.py: 185: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 186: copypaste: 0.4401,0.6537,0.4844,0.2954,0.4721,0.5640

####
test mask rcnn x152 training:
memory usage: 10GB per card, nearly 4 days remaining

####
fasterrcnn X101 with aug, 4gpu by --multi-gpu-testing, nearly 4 times speedup

####
what are the highlights of detectron? compared to keras faster-rcnn with resnet50 (box ap 34.8-3, no roi-align)
1. resnet50 modification  2ap
2. fpn 2ap
3. roi align  3ap
4. resnet 101, 152(on imagenet-5k)  2.5ap 5.5ap (compared to resnet50)
5. testing aug: h_flip, multi_scale, multi_ratio(not used) => box vote 3ap  (imporve small box recall and accuracy)
in total => 2+2+3+5.5+3=15.5  , around 47

####
mask rcnn without aug
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.398
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.635
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.423
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.199
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.430
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.574
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.322
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.495
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.517
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.317
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.561
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.678
 INFO json_dataset_evaluator.py: 122: Wrote json eval results to: ./test/coco_2014_minival/generalized_rcnn/segmentation_results.pkl
 INFO task_evaluation.py:  65: Evaluating segmentations is done!
 INFO task_evaluation.py: 180: copypaste: Dataset: coco_2014_minival
 INFO task_evaluation.py: 182: copypaste: Task: box
 INFO task_evaluation.py: 185: copypaste: AP,AP50,AP75,APs,APm,APl
 INFO task_evaluation.py: 186: copypaste: 0.4568,0.6695,0.4993,0.2870,0.4992,0.5774

only box ap is higher than resnet101, average recall/precision drops, don't know why
tried that, trigger box aug or box+mask aug cost almost same memory. Memroy will exhaust when scale's length is larger than 2
need to check whether faster rcnn model will allow for full augmentation on a 12 GB card.

####
test updates from facebook. cannot avoid memory failure
Also to remember that, once updated the code, cd lib to make to let it take effect.

####
test faster rcnn memory usage, a reasonable aug setting is     SCALES: (400, 600), need to check the performance, no h_flip for sacles
Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.426
Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.643
Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.463
Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.262
Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.463
Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.569
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.342
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.543
Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.574
Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.384
Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.614
Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.727
INFO json_dataset_evaluator.py: 199: Wrote json eval results to: ./test/coco_2014_minival/generalized_rcnn/detection_results.pkl
INFO task_evaluation.py:  61: Evaluating bounding boxes is done!
INFO task_evaluation.py: 180: copypaste: Dataset: coco_2014_minival
INFO task_evaluation.py: 182: copypaste: Task: box
INFO task_evaluation.py: 185: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 186: copypaste: 0.4263,0.6427,0.4629,0.2623,0.4628,0.5687

just get an averaged result compared to full-aug and no-aug. larger scales can imporve performance on small regions(guess)
(TODO) add another test, scales change to [1000, 1200]

 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.429
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.648
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.468
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.297
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.467
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.525
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.342
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.555
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.587
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.442
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.623
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.711
 INFO json_dataset_evaluator.py: 199: Wrote json eval results to: ./test/coco_2014_minival/generalized_rcnn/detection_results.pkl
 INFO task_evaluation.py:  61: Evaluating bounding boxes is done!
 INFO task_evaluation.py: 180: copypaste: Dataset: coco_2014_minival
 INFO task_evaluation.py: 182: copypaste: Task: box
 INFO task_evaluation.py: 185: copypaste: AP,AP50,AP75,APs,APm,APl
 INFO task_evaluation.py: 186: copypaste: 0.4286,0.6479,0.4685,0.2970,0.4668,0.5251
	    	 		     
### about memory
currently, only 1100, 1700 in X-101-32x8d is supported on frcnn. For maskrcnn and retinanet, the situation 
can be no better. So for training, only use low resolution images. a 2000,3333 sized images can be supported as tested.

### about retinanet. (trying)
I don't know whether this can improve performance of imbalanced pos vs. neg in cbis. Something related to this in paper
is about the difference between classes.
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.091
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.243
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.052
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.025
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.148
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.174
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.177
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.058
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.178
INFO json_dataset_evaluator.py: 199: Wrote json eval results to: ./test/cbis_ddsm_test/retinanet/detection_results.pkl
INFO task_evaluation.py:  61: Evaluating bounding boxes is done!
INFO task_evaluation.py: 180: copypaste: Dataset: cbis_ddsm_test
INFO task_evaluation.py: 182: copypaste: Task: box
INFO task_evaluation.py: 185: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 186: copypaste: 0.0912,0.2426,0.0522,-1.0000,0.0252,0.0914

the result is not ideal at all. And currently, we cannot use aug in retinanet test. This seems cannot improve the case
where fg and bg has a cirtical imbalance


### continue with faster rcnn ap test. X-152, no aug (on server)
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.358
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.583
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.391
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.211
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.401
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.447
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.301
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.479
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.504
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.321
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.545
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.616
INFO json_dataset_evaluator.py: 199: Wrote json eval results to: ./test/coco_2014_minival/generalized_rcnn/detection_results.pkl
INFO task_evaluation.py:  61: Evaluating bounding boxes is done!
INFO task_evaluation.py: 180: copypaste: Dataset: coco_2014_minival
INFO task_evaluation.py: 182: copypaste: Task: box
INFO task_evaluation.py: 185: copypaste: AP,AP50,AP75,APs,APm,APl
INFO task_evaluation.py: 186: copypaste: 0.3577,0.5833,0.3905,0.2110,0.4010,0.4471
'''
becasue the training not actually ended, to199999 iter, the result is worse than fasterrcn with x101
another guess is that, this doesn't improve a lot for fasterrcnn model
'''


### continue with faster rcnn ap test. X-152, aug, scale[1000,1200] with no h_flip
currently, due to unexpected termination, this test will be pending

### you can configure pixel means in core/config.py
### you can configure scales and maxsize in test and test_aug to use different and even original sizes in bbox prediction and classification

### in blob.py and minibatch.py
1. resize image due to scale and max_size, scale the shortest edge to SCALE, if the longest edge exceeds MAX_SIZE, scale it to MAX_SIZE
2. use the scaled image to feed the blob. the blob is in size [num_images, x_max, y_max, num_channels], those invalid regions are just filled in with zeros.
so for training and testing, larger scale and max_size will require for more params *in resnet, conv layer for rpn and conv layer before roialign*
if we use larger scale than that set in train, this also works as features can be correclty extracted.

In conclusion, for larger image size, during training, set SCALE to 1000, MAX_SIZE=1600. During test, set Scale to 600, 800, 1200, 1600, 2000 and MAX_SZIE=3200.
This should work for resnet-X101.
If acceptable, adjust scale to a larger number, say 2000, and keep max_size about 1.6 times scale.


(TDOO) configure channel mean for train	    	 		     
(TODO) to run a training on server, with scale 1100, max_size 1700. with 3 gpus, it might take about 1.5days.
(TODO) a frontend for detectron, priority => *low* maybe I can implement it at weekends
(TODO) with the trained model, test performance with only large scale, not for multi resolutions
if not possible, try to infer image with multiple splits. To my knowledge, even with poor resolution
during training, higher resolution test image will have better accuracy. write a script named split_simple_infer.py

to infer:
python tools/infer_simple.py --cfg configs/12_2017_baselines/e2e_faster_rcnn_X-101-32x8d-FPN_1x_cbis_local.yaml --output-dir tmp/detectron-visualizations --image-ext jpg test_imgs --wts model_files/fasterrcnn/X-101-32x8d-FPN_cbis_180000/model_final.pkl


for newly trained small model on filtered cbis, a larger scale does not produce better results.

## debug result. actually there's data leakage. anyway.
for a optimized configuration:
reson scale	result	                                       bbox aug	pre nms	post nms	nms	score_thres	rpn_nms_thres	box_aug_scale	h_flip	scale_h_flip	area_lo	area_hi	ratios	ratio_flip	vote	vote_score_m
final 800	0.0997,0.3018,0.0451,-1.0000,0.1010,0.0996     yes	1000	400		0.3		0.05	0.3		500	        FALSE	TRUE	         50	180	0.8	FALSE	        0.85	ID

result
INFO json_dataset_evaluator.py: 226: ~~~~ Mean and per-category AP @ IoU=[0.10,0.50] ~~~~
INFO json_dataset_evaluator.py: 227: 42.5
INFO json_dataset_evaluator.py: 235: 51.9
INFO json_dataset_evaluator.py: 235: 33.1
INFO json_dataset_evaluator.py: 236: ~~~~ Summary metrics ~~~~
Average Precision  (AP) @[ IoU=0.10:0.95 | area=   all | maxDets=100 ] = 0.251
Average Precision  (AP) @[ IoU=0.10      | area=   all | maxDets=100 ] = 0.524
Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.302
Average Precision  (AP) @[ IoU=0.10:0.95 | area= small | maxDets=100 ] = -1.000
Average Precision  (AP) @[ IoU=0.10:0.95 | area=medium | maxDets=100 ] = 0.281
Average Precision  (AP) @[ IoU=0.10:0.95 | area= large | maxDets=100 ] = 0.251
Average Recall     (AR) @[ IoU=0.10:0.95 | area=   all | maxDets=  1 ] = 0.351
Average Recall     (AR) @[ IoU=0.10:0.95 | area=   all | maxDets= 10 ] = 0.394
Average Recall     (AR) @[ IoU=0.10:0.95 | area=   all | maxDets=100 ] = 0.394
Average Recall     (AR) @[ IoU=0.10:0.95 | area= small | maxDets=100 ] = -1.000
Average Recall     (AR) @[ IoU=0.10:0.95 | area=medium | maxDets=100 ] = 0.278
Average Recall     (AR) @[ IoU=0.10:0.95 | area= large | maxDets=100 ] = 0.394
INFO json_dataset_evaluator.py: 203: Wrote json eval results to: ./test/cbis_ddsm_test_filtered/generalized_rcnn/detection_results.pkl
INFO task_evaluation.py:  61: Evaluating bounding boxes is done!
INFO task_evaluation.py: 180: copypaste: Dataset: cbis_ddsm_test_filtered
INFO task_evaluation.py: 182: copypaste: Task: box
INFO task_evaluation.py: 185: copypaste: AP,AP10,AP50,APs,APm,APl
INFO task_evaluation.py: 186: copypaste: 0.2512,0.5237,0.3018,-1.0000,0.2805,0.2514


